{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 폴더 경로 설정\n",
    "base_dir = './Fewshot'\n",
    "categories = ['Full', 'Part', 'Box', 'ETC']\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "# train 및 val 디렉토리 생성\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
    "\n",
    "# 데이터를 학습 및 검증 세트로 분할\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    images = os.listdir(category_dir)\n",
    "    train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(category_dir, img), os.path.join(train_dir, category, img))\n",
    "    \n",
    "    for img in val_images:\n",
    "        shutil.copy(os.path.join(category_dir, img), os.path.join(val_dir, category, img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "class FewShotDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, num_support=5, num_query=15):\n",
    "        self.dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
    "        self.num_support = num_support\n",
    "        self.num_query = num_query\n",
    "        self.transform = transform\n",
    "        self.class_indices = self._group_images_by_class()\n",
    "\n",
    "    def _group_images_by_class(self):\n",
    "        class_indices = {}\n",
    "        for idx, (img_path, label) in enumerate(self.dataset.imgs):\n",
    "            if label not in class_indices:\n",
    "                class_indices[label] = []\n",
    "            class_indices[label].append(img_path)\n",
    "        return class_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.classes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_label = list(self.class_indices.keys())[idx]\n",
    "        image_paths = self.class_indices[class_label]\n",
    "\n",
    "        # 이미지 수가 부족한 경우를 처리\n",
    "        if len(image_paths) < self.num_support + self.num_query:\n",
    "            # 부족한 경우 반복해서 채웁니다.\n",
    "            selected_images = np.random.choice(image_paths, self.num_support + self.num_query, replace=True)\n",
    "        else:\n",
    "            selected_images = np.random.choice(image_paths, self.num_support + self.num_query, replace=False)\n",
    "\n",
    "        support_images = selected_images[:self.num_support]\n",
    "        query_images = selected_images[self.num_support:]\n",
    "\n",
    "        support_set = [self.dataset.loader(img_path) for img_path in support_images]\n",
    "        query_set = [self.dataset.loader(img_path) for img_path in query_images]\n",
    "\n",
    "        if self.transform:\n",
    "            support_set = [self.transform(img) for img in support_set]\n",
    "            query_set = [self.transform(img) for img in query_set]\n",
    "\n",
    "        support_labels = [class_label] * self.num_support\n",
    "        query_labels = [class_label] * self.num_query\n",
    "\n",
    "        return torch.stack(support_set), torch.tensor(support_labels), torch.stack(query_set), torch.tensor(query_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 정의\n",
    "# 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_dir = 'Fewshot/train'\n",
    "val_dir = 'Fewshot/val'\n",
    "\n",
    "num_support = 5\n",
    "num_query = 15\n",
    "\n",
    "train_dataset = FewShotDataset(train_dir, transform=transform, num_support=num_support, num_query=num_query)\n",
    "val_dataset = FewShotDataset(val_dir, transform=transform, num_support=num_support, num_query=num_query)\n",
    "\n",
    "# 클래스 개수 가져오기\n",
    "num_classes = len(train_dataset.dataset.classes)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=num_support * num_classes, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=num_support * num_classes, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunwoong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\sunwoong/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:02<00:00, 45.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)  # ResNet50 사용\n",
    "        self.resnet.fc = nn.Identity()  # 마지막 FC 레이어 제거\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor, num_classes, num_support):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.num_classes = num_classes\n",
    "        self.num_support = num_support\n",
    "\n",
    "    def forward(self, support_images, query_images):\n",
    "        # 지원 세트에서 특징 추출\n",
    "        support_images = support_images.view(-1, *support_images.size()[2:])  # [num_classes * num_support, channels, height, width]\n",
    "        support_embeddings = self.feature_extractor(support_images)\n",
    "        \n",
    "        # 디버그: support_embeddings의 크기 출력\n",
    "        print(f\"support_embeddings shape: {support_embeddings.shape}\")\n",
    "        \n",
    "        # support_embeddings의 모양: [num_support * num_classes, embedding_size]\n",
    "        num_support_samples = self.num_support * self.num_classes\n",
    "        embedding_size = support_embeddings.size(-1)\n",
    "        \n",
    "        if support_embeddings.size(0) != num_support_samples:\n",
    "            raise ValueError(f\"Expected {num_support_samples} support samples, but got {support_embeddings.size(0)}\")\n",
    "        \n",
    "        support_embeddings = support_embeddings.view(self.num_classes, self.num_support, embedding_size)\n",
    "\n",
    "        # 프로토타입 계산\n",
    "        prototypes = support_embeddings.mean(dim=1)\n",
    "\n",
    "        # 질의 이미지에서 특징 추출\n",
    "        query_images = query_images.view(-1, *query_images.size()[2:])  # [num_query * num_classes, channels, height, width]\n",
    "        query_embeddings = self.feature_extractor(query_images)\n",
    "        \n",
    "        # 유클리드 거리 계산\n",
    "        dists = torch.cdist(query_embeddings, prototypes)\n",
    "        \n",
    "        # 최솟값 인덱스를 사용하여 클래스 예측\n",
    "        return dists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(train_dataset.dataset.classes)\n",
    "num_support = 5  # num_support 값을 설정\n",
    "model = PrototypicalNetwork(feature_extractor, num_classes, num_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [1/20], Loss: 1.6667711734771729\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 10.938091278076172, Accuracy: 15.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [2/20], Loss: 4.181957244873047\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 24.08255958557129, Accuracy: 10.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [3/20], Loss: 8.0247220993042\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 12.465987205505371, Accuracy: 31.666666666666668%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [4/20], Loss: 2.0576529502868652\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 14.25047492980957, Accuracy: 23.333333333333332%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [5/20], Loss: 2.2134997844696045\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 20.55222511291504, Accuracy: 20.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [6/20], Loss: 2.489922523498535\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 19.101192474365234, Accuracy: 16.666666666666668%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [7/20], Loss: 2.111503839492798\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 19.243566513061523, Accuracy: 15.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [8/20], Loss: 1.8810926675796509\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 7.303841590881348, Accuracy: 15.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [9/20], Loss: 2.075127601623535\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 3.1255643367767334, Accuracy: 20.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [10/20], Loss: 1.9627755880355835\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 9.330434799194336, Accuracy: 15.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [11/20], Loss: 1.496806263923645\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 3.3387181758880615, Accuracy: 13.333333333333334%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [12/20], Loss: 1.7560287714004517\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 3.5500648021698, Accuracy: 20.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [13/20], Loss: 1.8514333963394165\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 4.337718486785889, Accuracy: 21.666666666666668%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [14/20], Loss: 1.6363089084625244\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 2.381730318069458, Accuracy: 18.333333333333332%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [15/20], Loss: 2.0364348888397217\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 2.8519959449768066, Accuracy: 26.666666666666668%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [16/20], Loss: 1.5421416759490967\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 3.386352777481079, Accuracy: 8.333333333333334%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [17/20], Loss: 2.0751748085021973\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 3.189321994781494, Accuracy: 15.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [18/20], Loss: 1.763433814048767\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 3.6370503902435303, Accuracy: 11.666666666666666%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [19/20], Loss: 2.080620765686035\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 2.3951709270477295, Accuracy: 20.0%\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Epoch [20/20], Loss: 1.5157809257507324\n",
      "support_embeddings shape: torch.Size([20, 2048])\n",
      "Validation Loss: 4.118664264678955, Accuracy: 11.666666666666666%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for support_images, support_labels, query_images, query_labels in train_loader:\n",
    "        support_images = support_images.squeeze(0)\n",
    "        query_images = query_images.squeeze(0)\n",
    "        support_labels = support_labels.squeeze(0)\n",
    "        query_labels = query_labels.squeeze(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(support_images, query_images)\n",
    "\n",
    "        query_labels = query_labels.long()\n",
    "        query_labels = query_labels.view(-1)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "        loss = criterion(outputs, query_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()  # 학습률 조정\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for support_images, support_labels, query_images, query_labels in val_loader:\n",
    "            support_images = support_images.squeeze(0)\n",
    "            query_images = query_images.squeeze(0)\n",
    "            support_labels = support_labels.squeeze(0)\n",
    "            query_labels = query_labels.squeeze(0)\n",
    "\n",
    "            outputs = model(support_images, query_images)\n",
    "\n",
    "            query_labels = query_labels.long()\n",
    "            query_labels = query_labels.view(-1)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            loss = criterion(outputs, query_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += query_labels.size(0)\n",
    "            correct += (predicted == query_labels).sum().item()\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100 * correct/total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "model.load_state_dict(torch.load('shoe_classification_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def predict(image_bytes):\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return train_dataset.dataset.classes[predicted.item()]\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_route():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file provided'}), 400\n",
    "    \n",
    "    file = request.files['file']\n",
    "    img_bytes = file.read()\n",
    "    prediction = predict(img_bytes)\n",
    "    return jsonify({'category': prediction})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
